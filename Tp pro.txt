Create_token.py

from langchain.text_splitter import CharacterTextSplitter 

def chunk_ data(page):
try:
text _splitter = CharacterTextsplitter(
separator='In'
chunk size=9000,
I
chunk_overlap a 500,
metadata = [page.metadata] #storing meta data of each page to reuse
text_chunks = text_splitter.create_ documents ([page.page_content],metadatas metadata)
#uncomment the below comment to see the token size
#print(len(text_chunks [e].page_content)
return text_chunks
except Exception as e;
raise Exception(f"Error in chunk data: (str(e))")

load_file.py
import os
from langchain.document_loaders import PyPDFLoader
miuex.cSS
scri
def read_pdf(file_path):
#reading files
try:
loader - PyPDFLoader(file_path)
pages = loader.load_ and_split()
return pages
except Exception as e:
raise Exception(f"Error in reading PDF file: (str(e)]")
def load_files(directory_path):
#reading the whole directory
pdf files = [file for file in os.listdir(directory_path) if file.endswith('.pdf')]
content_list = []
for pdf_file in pdf_files:
file_path
os.path. join(directory_path,pdf_file)
content - read_ pdf(file_path)
I
content_list.append(content)
return content_list

openaiModel.py

import openai
import os
import ai_ component.open_ai_ key as open_ai_key
os.environ['OPENAI_ API_ KEY'] = open_ai_key.mykey
from create _logs import log
def get_ response(input,docsearch_index): #function to connect wiht openai
try:
docs = docsearch_ index.similarity_ search(input,k=2)
#uncomment the below comment to see the meta data associated the similar results
#print(docs[e])
source = (]
extracted result
for doc in docs:
extracted result+-doc.page_content[:]
if doc.metadata['source'] in source:
source[doc.metadata[' source' ]].append(doc.metadata['page'])
else:
source[doc.metadata[' source` ]]=[doc.metadata["page`]]
print(source)
#
prompt = f"""
You are an AI assistant. Answer the user queries using only the below information delimited in tripl
Information: `'[extracted_ result]
messages= [
I
['role':'system','content':prompt),
('role':'user','content':input)

chat = openai. ChatCompletion.create(
model='gpt-3.5-turbo'
messages = messages,
n=3
chat['source'] = source
print(chat)
log(input,chat) #saving the log
# reply = chat.choices[0].message.content
answers = []
for ans in chat.choices:
answers.append(ans.message.content)
return [answers,chat['source']]
except Exception as e:
raise Exception(f'Error while loading get_response fstr(e))")

vector_db_con.py

import pinecone
import ai_ component.pinecone asset as pinecone_asset
from langchain.vectorstores import Pinecone
from ai_component.create_token import chunk_data
from ai_component.load_file import load_files
index_name = 'project1'
dimension = 1536

def connect_ pinecone():
try:
pinecone.init(
api key = pinecone_asset.api_key,
environment = pinecone _asset.environment
print("Connected with Pinecone successfully!")
except Exception as e:
raise Exception(f'Error while connecting with pinecone (str(e)]).')
def docsearch(directory_path,embeddings):
try:
content _list = load_files(directory_path) #loading
document = []
for content in content list:
for page in content:
document.extend(chunk_data(page))
* print(document[0])
if index_name not in pinecone.list_indexes():
print(`Creating the index..
I
pinecone.create_index(index name,dimension)
print("Index Created\nIntialising
return Pinecone.from_ documents (document, embeddings, index_name = index_name)
else:
print(" Index already exists\nLoading the index.
return Pinecone.from_existing_ index(index_ name, embeddings)
except Exception as e:
raise Exception[f'Error while creating/ fetching data from pinecone (str(e)]).")

app.py
import ai_component.vector_db_con as vector_db_con
from langchain.embeddings.openai import OpenAIEmbeddings
from ai_ component.openaiModel import get_response as response
openaiModel.py
index.html
from flask import Flask, render_ template, request
app = Flask(_name
@app.route('/`)
def home():
return render_ template('index.html')
@app.route('/get_response', methods=['POST'])
def get_response():
# Process user input and generate a response
user_input = request.form['user_ input']
res = response(user input,docsearch)
return ('response': res[0], 'source': res[1]]
@app.route('/save_report', methods=['POST'])
def save_ report():
report = request.data.decode('utf-8')
with open('./reports/report.txt', 'a') as file:
file.write(report + 'In')
return 'Report saved successfully.

directory_ path = './files'
embeddings = OpenAIEmbeddings()
#this embeddin
vector _db_con.connect_ pinecone()
#calling the c
docsearch = vector_db_con.docsearch(directory_path,e
if
name_== '_ main_':
app.run()

Create_logs.py

import logging
import time
current time = time.strftime( "%Y-%m-%d_%H-%M-%s", time.localtime())
log_filename = f"session_ (current _time).txt"
# Configure logging
logging.basicConfig(filename=f'./logs/(log filename)', level=logging.INFO, forma
def log(input,result):
# Function to log input and return result
# Log input and result
logging.info(f"Input: (input),In Result: (result)")

open mykey = 'sk-XOotgpQgNYqc9uN12vohT2BlbkFJAXRfuAjekOxogqf2LFnO' 'org-zsTAYAsDjYT7TtMAZ3PRAZdw'
pc api_key = "ab5450ca-e053-4454-b8bd-7ed69c6d5693"
environment = "us-west1-gcp-free"